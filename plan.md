# 带情绪解析的本地语音聊天助手开源方案（Ubuntu22 + 3090）

整体架构升级为 5大核心模块，新增语音情绪识别（SER）模块，实现“语音输入→文字转录+情绪解析→带情绪上下文的智能回复→情绪匹配语音输出”的闭环，全程开源、离线部署，适配3090单卡，无额外成本。

# 一、核心架构与模块选型

|模块|核心功能|推荐开源模型/工具（适配3090）|选型理由（低成本+易部署）|
|---|---|---|---|
|语音采集|接收用户语音输入（麦克风/音频文件）|PyAudio、SoundDevice|轻量Python库，无GPU依赖，直接采集音频流|
|语音转文字（STT）|语音内容精准转录为文本|Whisper (OpenAI) - large-v3（8-bit量化）|开源免费，多语言支持，3090推理快，转录准确率高|
|语音情绪识别（SER）|从语音声学特征解析用户语气/情绪（开心/愤怒/悲伤/中性等）|wav2vec2-base-emo、CNN-RNN SER轻量版|模型体积小（<500MB），3090毫秒级推理，直接输出情绪标签|
|对话大模型（LLM）|结合文本内容+情绪标签+对话历史生成带对应语气的回复|Llama 3 8B Instruct（4-bit量化）、Qwen 7B Chat|单卡可运行，支持上下文窗口，能根据情绪标签调整回复语气|
|文字转语音（TTS）|将带情绪的回复文本转为匹配语气的语音|Coqui TTS（emotion-tts模型）、VITS|开源免费，支持情绪语音生成（如愤怒→严肃语调，开心→轻快语调）|
|上下文+情绪管理|维护对话历史+对应轮次情绪标签|本地JSON文件缓存|零成本，无需数据库，直接存储“用户文本+情绪+助手回复”|
# 二、各模块详细部署要点

## 1. 语音采集

- 工具选择：PyAudio（简单易用）或 SoundDevice（低延迟）；

- 部署：直接通过Python调用麦克风，将采集的音频保存为wav格式（16kHz采样率，与Whisper/SER模型输入要求一致）；

- 优化：设置音频缓存阈值（如采集到2秒静音自动停止），避免无效音频输入。

## 2. STT + SER 联动：同步解析文字+情绪

核心逻辑：同一份用户音频文件，并行输入Whisper和SER模型，同时输出文本内容和情绪标签，无需二次采集。

### （1）语音转文字（Whisper）

- 模型版本：whisper-large-v3 8-bit量化版（3090显存占用约4GB）；

- 部署：通过openai-whisper库调用，开启CUDA加速，转录60秒语音仅需2~3秒；

- 优化：指定语言为中文（language="Chinese"），减少多语言识别干扰，提升准确率。

### （2）语音情绪识别（SER）

- 模型选择（二选一）：
        

    - wav2vec2-base-emo（Hugging Face开源）：基于wav2vec2轻量版训练，支持识别4类基础情绪（happy/angry/sad/neutral），显存占用<1GB，3090推理单段音频<100ms；

    - CNN-RNN SER轻量版：专为消费级GPU设计，模型体积更小（<200MB），适合对延迟要求高的场景；

- 部署：通过transformers库加载模型，输入与Whisper相同的wav音频（无需格式转换），输出情绪标签及置信度（如angry: 0.92）；

- 优化：设置置信度阈值（如>0.7才生效），低于阈值则标记为neutral（中性），避免误判。

## 3. 对话大模型（LLM）：情绪+上下文驱动回复

这是实现“语气适配”的核心环节，关键在于构造带情绪标签的Prompt。

### （1）模型选择

- 首选：Llama 3 8B Instruct 4-bit量化版（显存占用约6GB）：Meta开源，上下文窗口8k，支持情绪指令；

- 备选：Qwen 7B Chat 4-bit量化版（显存占用约5GB）：中文适配更好，对情绪描述的理解更精准；

- 部署工具：llama.cpp（纯C++实现，低显存占用）或transformers + bitsandbytes（Python生态友好），均支持CUDA加速。

### （2）Prompt构造模板（核心）

将“对话历史+当前文本+情绪标签”整合为Prompt，强制LLM根据情绪调整回复语气，示例：

> 你是一个贴心的语音聊天助手，需要根据用户的情绪调整回复语气：
> 
> 1. 若用户情绪为angry（愤怒）：回复需温和、安抚，避免激化矛盾；
> 
> 2. 若用户情绪为happy（开心）：回复需热情、轻快，呼应用户情绪；
> 
> 3. 若用户情绪为sad（悲伤）：回复需共情、鼓励；
> 
> 4. 若用户情绪为neutral（中性）：回复需自然、简洁。
> 
> 
> 对话历史：
> 
> 用户（neutral）：你好
> 
> 助手：你好呀！有什么可以帮你的吗？
> 
> 
> 当前用户情绪：angry
> 
> 当前用户输入：这个功能怎么一直用不了！
> 
> 请结合对话历史和情绪，生成合适的回复：
> 
> 

### （3）上下文管理

- 存储方式：本地JSON文件，每条记录包含轮次、用户文本、用户情绪、助手回复、时间戳；

- 优化：限制历史轮次（如最多保留10轮），避免Prompt过长导致显存溢出；超过阈值时，自动截断最早的对话轮次。

## 4. TTS：生成匹配情绪的语音

核心目标：让语音语调与LLM回复的语气一致，避免“文字温和但语音生硬”的割裂感。

### （1）模型选择

- 首选：Coqui TTS - emotion-tts模型（开源中文情绪语音模型）：支持直接传入情绪标签（如--emotion angry），自动调整语音的音调、语速、音量；

- 备选：VITS 轻量版：可下载开源的中文情绪音色模型（如“开心女声”“中性男声”），根据SER输出的情绪标签切换对应音色；

- 部署：显存占用约2~3GB，3090生成10秒语音<500ms，支持离线运行。

### （2）情绪联动逻辑

|用户SER情绪标签|TTS 语气配置|示例|
|---|---|---|
|angry|降低语速、调低音调、温和语调|缓慢、耐心的语音|
|happy|加快语速、调高音调、轻快语调|活泼、热情的语音|
|sad|放缓语速、低沉音调、共情语调|温柔、安慰的语音|
|neutral|正常语速、平稳语调|自然、流畅的语音|
# 三、整体流程闭环

用户麦克风输入语音 → PyAudio保存为wav音频

    ↓

    音频并行输入Whisper + SER模型

    ↓

    Whisper输出「用户文本」，SER输出「情绪标签」

    ↓

    加载本地JSON对话历史 → 构造带情绪的Prompt

    ↓

    LLM生成带对应语气的回复文本 → 更新JSON对话历史

    ↓

    回复文本 + 情绪标签输入TTS模型 → 生成匹配语气的语音

    ↓

    通过扬声器播放语音给用户

# 四、3090 资源占用与优化（关键）

|模块|量化后显存占用|合计显存占用|
|---|---|---|
|Whisper large-v3（8-bit）|~4GB|合计约14GB|
|wav2vec2-base-emo|~1GB||
|Llama 3 8B（4-bit）|~6GB||
|Coqui TTS|~3GB||
3090 有24GB显存，剩余10GB可用于音频缓存和系统开销，完全足够流畅运行。

### 额外优化技巧

- 模型按需加载：非对话时段卸载LLM/TTS模型，节省显存；

- 批量推理：短语音合并处理，减少模型调用次数；

- 关闭无关进程：Ubuntu下关闭浏览器、视频播放器等，释放GPU资源。

# 五、低成本关键保障

- 零云端费用：所有模型本地离线运行，无需调用API；

- 无额外硬件成本：仅用现有3090+Ubuntu，无需新增设备；

- 开源免费：所有模型/工具均为开源协议，无商业授权费用；

- 低部署复杂度：无需分布式架构，单卡单进程即可实现闭环。

# 六、补充说明

1. 模型获取：所有推荐模型均可通过Hugging Face官网或国内镜像站下载，需提前注册账号（免费）；

    2. 环境依赖：需提前安装NVIDIA驱动（适配3090）+ CUDA 11.8/12.0，Python 3.9+及对应依赖库（torch、transformers等）；

    3. 后续优化方向：可微调TTS情绪模型适配个人音色，或扩展SER模型支持更多细分情绪（如惊讶、焦虑）。
